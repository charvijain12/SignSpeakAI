# SignSpeakAI - Gesture Recognition using ASL ğŸ‘

## Introduction ğŸš€

Gesture Recognition using ASL (American Sign Language) is a project designed for recognizing hand gestures and interpreting them as letters from the American Sign Language alphabet. Leveraging Convolutional Neural Networks (CNN), this application aims to bridge communication gaps for individuals who use sign language.

## Key Features ğŸŒŸ

- **CNN Model:** Utilizes a Convolutional Neural Network for accurate gesture recognition ğŸ§ 
- **Training and Evaluation:** The model is trained on a sign language dataset and evaluated for accuracy ğŸ“ˆ
- **Visualization:** Confusion matrix and classification report provide insights into the model's performance ğŸ“Š
- **User Interaction:** Enables users to interact with the system through gestures ğŸ¤–


## Getting Started ğŸ

### Prerequisites ğŸ› ï¸

- Python
- TensorFlow
- Pandas
- Matplotlib
- Seaborn
- Scikit-learn

### Installation ğŸ“¦

1. Clone the repository:

   ```bash
   git clone https://github.com/yourusername/Gesture-Recognition-ASL.git
   ```

2. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

## Usage ğŸ–¥ï¸

1. Load the ASL gesture dataset.
2. Train the CNN model using the provided script.
3. Evaluate the model's performance.
4. Interact with the system using hand gestures.

## Results ğŸ“Š

The CNN model achieves an accuracy of 94% on the test set, providing reliable recognition of American Sign Language gestures.

## Contributing ğŸ¤

Contributions are welcome! Feel free to enhance the project, add new features, or improve existing ones. Follow the guidelines in the Contributing file.

## License ğŸ“„

This project is licensed under the [MIT License](https://github.com/charvijain12/SignSpeakAI/blob/main/LICENSE).
